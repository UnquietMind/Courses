
# 卡耐基梅隆大学 2018 秋季《深度学习导论》，Bhiksha Raj
### http://deeplearning.cs.cmu.edu/

## [Lecture1.pdf](/Lecture%201.pdf)
> Introduction to deep learning <br>
> Course logistics <br>
> History and cognitive basis of neural computation <br>
> The perceptron / multi-layer perceptron <br>

## [Lecture2.pdf](/Lecture%202.pdf) 
> The neural net as a universal approximator

## [Lecture3.pdf](/Lecture%203.pdf) 
> Training a neural network <br>
> Perceptron learning rule <br>
> Empirical Risk Minimization <br>
> Optimization by gradient descent

## [Lecture4.pdf](/Lecture%204.pdf) 
> Back propagation <br>
> Calculus of back propogation

## [Lecture5.pdf](/Lecture%205.pdf) 
> Backprop Continued

## [Lecture6.pdf](/Lecture%206.pdf) 
> Convergence in neural networks <br>
> Rates of convergence <br>
> Loss surfaces <br>
> Learning rates, and optimization methods <br>
> RMSProp, Adagrad, Momentum

## [Lecture7.pdf](/Lecture%207.pdf) 
> Stochastic gradient descent <br>
> Acceleration <br>
> Overfitting and regularization <br>
> Tricks of the trade: <br>
- Choosing a divergence (loss) function <br>
- Batch normalization <br>
- Dropout

## [Lecture8.pdf](/Lecture%208.pdf) 
> Convolutional Neural Networks (CNNs) <br>
> Weights as templates <br>
> Translation invariance <br>
> Training with shared parameters <br>
> Arriving at the convlutional model

## [Lecture9.pdf](/Lecture%209.pdf) 
> Models of vision <br>
> Neocognitron <br>
> Mathematical details of CNNs <br>
> Alexnet, Inception, VGG

## [Lecture10.pdf](/Lecture%2010.pdf) 
> Recurrent Neural Networks (RNNs) <br>
> Modeling series <br>
> Back propogation through time <br>
> Bidirectional RNNs

## [Lecture11.pdf](/Lecture%2011.pdf) 
> Stability <br>
> Exploding/vanishing gradients <br>
> Long Short-Term Memory Units (LSTMs) and variants <br>
> Resnets

## [Lecture12.pdf](/Lecture%2012.pdf) 
> Loss functions for recurrent networks <br>
> Sequence prediction

## [Lecture13.pdf](/Lecture%2013.pdf) 
> Sequence To Sequence Methods <br>
> Connectionist Temporal  <br>
> Classification (CTC)

## [Lecture14.pdf](/Lecture%2014.pdf) 
> Sequence-to-sequence models <br>
> Examples from speech and language

## [Lecture15.pdf](/Lecture%2015.pdf) 
> Attention

## [Lecture16.pdf](/Lecture%2016.pdf) 
> Cascade Correlation

## [Lecture17.pdf](/Lecture%2017.pdf) 
> What to networks represent <br>
> Autoencoders and dimensionality reduction <br>
> Learning representations

## [Lecture18.pdf](/Lecture%2018.pdf) 
> Variational Autoencoders (VAEs)

## [Lecture19.pdf](/Lecture%2014.pdf) 
> Generative Adversarial Networks (GANs) Part 1

## [Lecture20.pdf](/Lecture%2017.pdf) 
> Generative Adversarial Networks (GAN s) Part 2 <br>
> Hopfield Networks <br>
> Boltzmann Machines

## [Lecture21.pdf](/Lecture%2017.pdf) 
> Training Hopfield Networks <br>
> Stochastic Hopfield Networks

## [Lecture22.pdf](/Lecture%2017.pdf) 
> Restricted Boltzman Machines <br>
> Deep Boltzman Machines

## [Lecture23.pdf](/Lecture%2017.pdf) 
> Reinforcement Learning 1

## [Lecture24.pdf](/Lecture%2017.pdf) 
> Reinforcement Learning 2

## [Lecture25.pdf](/Lecture%2017.pdf) 
> Thanksgiving Break - No Classes

## [Lecture26.pdf](/Lecture%2017.pdf) 
> Reinforcement Learning 3

## [Lecture27.pdf](/Lecture%2017.pdf) 
> Reinforcement Learning 4

## [Lecture28.pdf](/Lecture%2017.pdf) 
> Q Learning <br>
> Deep Q Learning

## [Lecture29.pdf](/Lecture%2017.pdf) 
> Newer models and trends <br>
> Review
